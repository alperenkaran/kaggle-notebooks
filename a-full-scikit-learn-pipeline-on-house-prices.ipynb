{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/alperenkaran/a-full-scikit-learn-pipeline-on-house-prices?scriptVersionId=88886380\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.083004Z","iopub.execute_input":"2021-08-01T22:21:33.083359Z","iopub.status.idle":"2021-08-01T22:21:33.509642Z","shell.execute_reply.started":"2021-08-01T22:21:33.083279Z","shell.execute_reply":"2021-08-01T22:21:33.508746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 1: Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#### Read the files","metadata":{}},{"cell_type":"code","source":"folder = '../input/house-prices-advanced-regression-techniques/'\n\ntrain = pd.read_csv(folder + 'train.csv')\ntest = pd.read_csv(folder + 'test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.51118Z","iopub.execute_input":"2021-08-01T22:21:33.511604Z","iopub.status.idle":"2021-08-01T22:21:33.574035Z","shell.execute_reply.started":"2021-08-01T22:21:33.511576Z","shell.execute_reply":"2021-08-01T22:21:33.573224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create `x_train`, `y_train` and `x_test`","metadata":{}},{"cell_type":"code","source":"x_train = train.drop(['SalePrice', 'Id'], axis=1)\ny_train = np.log(train['SalePrice'])\nx_test = test.drop('Id', axis=1)\nx_test_id = test['Id']\ndel train, test","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.575344Z","iopub.execute_input":"2021-08-01T22:21:33.57577Z","iopub.status.idle":"2021-08-01T22:21:33.585757Z","shell.execute_reply.started":"2021-08-01T22:21:33.575738Z","shell.execute_reply":"2021-08-01T22:21:33.584926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.587109Z","iopub.execute_input":"2021-08-01T22:21:33.587715Z","iopub.status.idle":"2021-08-01T22:21:33.62468Z","shell.execute_reply.started":"2021-08-01T22:21:33.587669Z","shell.execute_reply":"2021-08-01T22:21:33.623674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the column `MSSubClass` is actually categorical, we need to convert it from numerical to string.","metadata":{}},{"cell_type":"code","source":"x_train['MSSubClass'] = x_train['MSSubClass'].astype(str)\nx_test['MSSubClass'] = x_test['MSSubClass'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.625982Z","iopub.execute_input":"2021-08-01T22:21:33.626548Z","iopub.status.idle":"2021-08-01T22:21:33.637045Z","shell.execute_reply.started":"2021-08-01T22:21:33.626503Z","shell.execute_reply":"2021-08-01T22:21:33.636275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 2: When there are too many missing values\n\nSome columns may have a few missing values, and some columns can be full of missing values.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.hist(x_train.isnull().mean(), bins=20)\nplt.title('Histogram of percent missing values in x_train', fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.638218Z","iopub.execute_input":"2021-08-01T22:21:33.63878Z","iopub.status.idle":"2021-08-01T22:21:33.848429Z","shell.execute_reply.started":"2021-08-01T22:21:33.638739Z","shell.execute_reply":"2021-08-01T22:21:33.847319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By looking at the above histogram, maybe it is a good idea, if we delete the columns which have 40% or more missing values.","metadata":{}},{"cell_type":"code","source":"missing_cols = x_train.columns[x_train.isnull().mean() > .4]\n\nx_test = x_test.loc[:,~x_train.columns.isin(missing_cols)]\nx_train = x_train.loc[:,~x_train.columns.isin(missing_cols)]\n\nprint('The columns with a large number of missing values are:', list(missing_cols))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.849683Z","iopub.execute_input":"2021-08-01T22:21:33.850263Z","iopub.status.idle":"2021-08-01T22:21:33.876872Z","shell.execute_reply.started":"2021-08-01T22:21:33.850217Z","shell.execute_reply":"2021-08-01T22:21:33.876001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 3: Split Numerical and Categorical columns\n\nWe create two lists, containing the column names of categorical and numerical variables.","metadata":{}},{"cell_type":"code","source":"num_cols, cat_cols = [], []\n\nfor col in x_train.columns:\n    if x_train[col].dtype == object:\n        cat_cols.append(col)\n    else:\n        num_cols.append(col)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.879355Z","iopub.execute_input":"2021-08-01T22:21:33.88008Z","iopub.status.idle":"2021-08-01T22:21:33.888708Z","shell.execute_reply.started":"2021-08-01T22:21:33.880037Z","shell.execute_reply":"2021-08-01T22:21:33.887975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are', len(cat_cols), 'categorical columns.')\nprint('There are', len(num_cols), 'numerical columns.')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.890322Z","iopub.execute_input":"2021-08-01T22:21:33.890926Z","iopub.status.idle":"2021-08-01T22:21:33.905319Z","shell.execute_reply.started":"2021-08-01T22:21:33.89088Z","shell.execute_reply":"2021-08-01T22:21:33.90432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 4: Categorical Pipeline\n\nFirst, we will impute the missing values by filling the columns with most frequent value. Then we will one-hot-encode the columns.\n\nLet's put the two in a pipeline called `cat_pipeline`.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_pipeline = Pipeline([('imputer_mode', SimpleImputer(strategy='most_frequent')),\n                         ('one_hot_encoder', OneHotEncoder(handle_unknown = \"ignore\"))])","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:33.906652Z","iopub.execute_input":"2021-08-01T22:21:33.906974Z","iopub.status.idle":"2021-08-01T22:21:34.010137Z","shell.execute_reply.started":"2021-08-01T22:21:33.906933Z","shell.execute_reply":"2021-08-01T22:21:34.009041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 5: Numerical Pipeline\n\nThis time we will impute the missing values with the mean of the columns. Then, we will apply a standard scaler on these features.\n\nAgain, let's put these transformations in a pipeline called `num_pipeline`.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([('imputer_mean', SimpleImputer(strategy='mean')),\n                         ('scaler', StandardScaler())])","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:34.011423Z","iopub.execute_input":"2021-08-01T22:21:34.011753Z","iopub.status.idle":"2021-08-01T22:21:34.018216Z","shell.execute_reply.started":"2021-08-01T22:21:34.01172Z","shell.execute_reply":"2021-08-01T22:21:34.017057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 6: Merge Categorical and Numerical Pipelines\n\nWe already know what features are categorical and what features are numerical. We want to build a transformer, which runs categorical columns in the `cat_pipeline`, numerical columns in the `num_pipeline`, and finally merges them into a big dataset.\n\nLuckily, scikit-learn's `ColumnTransformer` does this job pretty well. We name this transformation `col_transformer`.","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\ncol_transformer = ColumnTransformer(transformers=[('cat_transformer', cat_pipeline, cat_cols),\n                                                  ('num_transformer', num_pipeline, num_cols)])","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:34.019769Z","iopub.execute_input":"2021-08-01T22:21:34.020192Z","iopub.status.idle":"2021-08-01T22:21:34.034672Z","shell.execute_reply.started":"2021-08-01T22:21:34.020083Z","shell.execute_reply":"2021-08-01T22:21:34.033619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 7: Full pipeline\n\nNow that we have a `col_transformer` that takes the raw data as input, and spits out a imputed and processed data, we can put it into a pipeline which ends with an estimator.\n\nUsually, xgboost is a great choice for regression tasks. So, let's build our last pipeline, which consists of the `col_transformer` and `xgb_regressor`.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(max_depth=7, n_estimators=500, eta=.1, random_state=0)\n\nfull_pipeline = Pipeline([('col_transformer', col_transformer),\n                          ('xgb_regressor', xgb)])","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:34.035997Z","iopub.execute_input":"2021-08-01T22:21:34.036395Z","iopub.status.idle":"2021-08-01T22:21:34.060212Z","shell.execute_reply.started":"2021-08-01T22:21:34.03636Z","shell.execute_reply":"2021-08-01T22:21:34.059154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 8: Cross validation\n\nNow that we have finalized our pipeline, we can use scikit-learn's `cross_val_score` for an estimation for the errors.\n\nKaggle evaluates the results based on __root mean squared error__ of the log-labels. We have already computed the logarithms of the labels, and written them in `y_train`. \n\nScikit-learn always wants its scorer functions to produce higher values from better models. However, this is not the case for `root_mean_squared_error`. _i.e._, better models imply lower root mean squared error. This is why scikit-learn does not have a built-in `root_mean_squared_error`, but instead, they use `neg_root_mean_squared_error` (neg means negative).\n\nTo get the actual root mean squared error, we simply multiply the result with -1.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nresult = cross_val_score(full_pipeline, x_train, y_train, scoring='neg_root_mean_squared_error')\n\nprint('The cross validation score is', np.mean(-result))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:34.061616Z","iopub.execute_input":"2021-08-01T22:21:34.06195Z","iopub.status.idle":"2021-08-01T22:21:43.655811Z","shell.execute_reply.started":"2021-08-01T22:21:34.061919Z","shell.execute_reply":"2021-08-01T22:21:43.654756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 9: Submission\n\nNot we create our submission file. Note the we made our predictions using log-labels, so we need to convert them back.","metadata":{}},{"cell_type":"code","source":"full_pipeline.fit(x_train, y_train)\n\ny_test_predicted = full_pipeline.predict(x_test)\n\nsubmission = pd.DataFrame()\nsubmission['Id'] = x_test_id\nsubmission['SalePrice'] = np.exp(y_test_predicted) #converted to original prices\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T22:21:43.657283Z","iopub.execute_input":"2021-08-01T22:21:43.65764Z","iopub.status.idle":"2021-08-01T22:21:46.16012Z","shell.execute_reply.started":"2021-08-01T22:21:43.657606Z","shell.execute_reply":"2021-08-01T22:21:46.159107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Section 10: Future Work\n\nThe results can further be improved if we try some of the following steps:\n\n- Different hyperparameters for xgboost estimator (e.g. max_depth=10 instead of max_depth=7)\n- Different estimators (e.g. RandomForestRegressor instead of XGBRegressor)\n- We can put a feature selector (e.g. VarianceThreshold) immediately after the `col_transformer`.\n- Lastly, but most importantly, we can do some actualy feature engineering. \n    - Example 1: the column `LotShape` has four values: `Reg`, `IR1`, `IR2`, and `IR3` (which stands for Regular, slightly irregular, ..., most irregular). Instad of one-hot-encoding this categorical variable, we could transform it to a binary numerical variable (1=Regular, 0=Irregular).\n    - Example 2: The columns `LotFrontage`, `GrLivArea` etc. are right-skewed. A log-transformation can yield better performances.","metadata":{}}]}