{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/alperenkaran/why-you-shouldn-t-train-test-split-small-datasets?scriptVersionId=88886119\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, I am going to explain why it is a bad idea to train-test split a small dataset.","metadata":{}},{"cell_type":"markdown","source":"# Read the data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:25.004858Z","iopub.execute_input":"2021-08-09T18:20:25.005274Z","iopub.status.idle":"2021-08-09T18:20:25.976354Z","shell.execute_reply.started":"2021-08-09T18:20:25.005171Z","shell.execute_reply":"2021-08-09T18:20:25.975339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/iris/Iris.csv')\ndata.sample(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:25.981789Z","iopub.execute_input":"2021-08-09T18:20:25.98219Z","iopub.status.idle":"2021-08-09T18:20:26.034932Z","shell.execute_reply.started":"2021-08-09T18:20:25.982152Z","shell.execute_reply":"2021-08-09T18:20:26.033744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `Id` column is useless, we delete it.\n\nThe `Species` contains the labels for the machine learning task. ","metadata":{}},{"cell_type":"code","source":"x = data.drop(['Id','Species'], axis=1) #the features are in x\ny = data['Species'] #the labels are in y","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:26.036656Z","iopub.execute_input":"2021-08-09T18:20:26.037019Z","iopub.status.idle":"2021-08-09T18:20:26.044696Z","shell.execute_reply.started":"2021-08-09T18:20:26.036983Z","shell.execute_reply":"2021-08-09T18:20:26.043791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The data has', len(data), 'rows.')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:26.84016Z","iopub.execute_input":"2021-08-09T18:20:26.840551Z","iopub.status.idle":"2021-08-09T18:20:26.846741Z","shell.execute_reply.started":"2021-08-09T18:20:26.840517Z","shell.execute_reply":"2021-08-09T18:20:26.8458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's draw a scatterplot","metadata":{}},{"cell_type":"markdown","source":"The dataset have originally four features: `SepalLengthCm`, `SepalWidthCm`, `PetalLengthCm`, and `PetalWidthCm`.\n\nLet's plot `SepalWidthCm` against `PetalWidthCm`.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7,7))\nsns.scatterplot(x=x['SepalWidthCm'], y=x['PetalWidthCm'], hue=y, s=80)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:27.771085Z","iopub.execute_input":"2021-08-09T18:20:27.771449Z","iopub.status.idle":"2021-08-09T18:20:28.056382Z","shell.execute_reply.started":"2021-08-09T18:20:27.771413Z","shell.execute_reply":"2021-08-09T18:20:28.055025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear that the blue points are very easy to classify, and the errors (if any) are expected to occur between the orange and green points.\n\nIn particular, if we only used these two features (Sepal width and Petal width), the misclassification errors would appear at the points highlighted in thick black boundary below.","metadata":{}},{"cell_type":"code","source":"difficult_points = [54,68,72,77,78,83,119,129,133,134]\nline_widths = [2 if i in difficult_points else .01 for i in range(150)]\n\nplt.figure(figsize=(7,7))\nsns.scatterplot(x=x['SepalWidthCm'], y=x['PetalWidthCm'], hue=y, s=80, edgecolor='black', linewidth=line_widths)\nplt.title('Points with black boundary seems to be hard to classify', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:29.058774Z","iopub.execute_input":"2021-08-09T18:20:29.059093Z","iopub.status.idle":"2021-08-09T18:20:29.294225Z","shell.execute_reply.started":"2021-08-09T18:20:29.059065Z","shell.execute_reply":"2021-08-09T18:20:29.293287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What happens if you do a train-test split?\n\n- If you use 20% of your data for testing, then actually you are sparing 150*0.20 = 30 data points for test. This means that, every misclassification will cost you 3.33% accuracy.\n- If you are lucky enough, then your test set would not contain any of the highlighted (black boundary) points above, and you would get 100% accuracy.","metadata":{}},{"cell_type":"markdown","source":"# Try with a bunch of different train-test splits\n\nWe will control the randomness of the split via the `random_state` argument in the `train_test_split` function below.\n\nWe create a pipeline where the input is standardized first, then fed into the svm classifier.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\n\naccuracies = []\n\nfor i in range(500):\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=i) #we control the random_state parameter\n    pipeline = make_pipeline(StandardScaler(), SVC(random_state=0)) #first we scale, then run our classifier\n    pipeline.fit(x_train, y_train)\n    accuracy = pipeline.score(x_test, y_test)\n    accuracies.append(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:30.68506Z","iopub.execute_input":"2021-08-09T18:20:30.685441Z","iopub.status.idle":"2021-08-09T18:20:34.457917Z","shell.execute_reply.started":"2021-08-09T18:20:30.685408Z","shell.execute_reply":"2021-08-09T18:20:34.457067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_table = pd.Series(accuracies).value_counts().sort_index().reset_index()\nfreq_table.columns = ['accuracy','frequency']\nfreq_table","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:40:06.747761Z","iopub.execute_input":"2021-08-09T18:40:06.748432Z","iopub.status.idle":"2021-08-09T18:40:06.761595Z","shell.execute_reply.started":"2021-08-09T18:40:06.748382Z","shell.execute_reply":"2021-08-09T18:40:06.760662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, it appears that\n- The accuracies indeed change by 3.33%\n- The accuracy on the test set highly depends on the random state","metadata":{}},{"cell_type":"markdown","source":"__Question:__ Why doesn't this happen in large datasets?\n\n__Answer:__ It does. But since the test set in large datasets is also large, the effect of the random state is negligible.","metadata":{}},{"cell_type":"markdown","source":"# Demonstration","metadata":{}},{"cell_type":"code","source":"# random_state = 0\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.20, random_state=0)\npipeline = make_pipeline(StandardScaler(), SVC(random_state=0)) #first we scale, then run our classifier\npipeline.fit(x_train, y_train)\naccuracy = pipeline.score(x_test, y_test)\n\nprint('Accuracy for random_state=0 is',str(accuracy*100) +'%')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:35.736904Z","iopub.execute_input":"2021-08-09T18:20:35.737271Z","iopub.status.idle":"2021-08-09T18:20:35.755423Z","shell.execute_reply.started":"2021-08-09T18:20:35.737238Z","shell.execute_reply":"2021-08-09T18:20:35.754356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random_state = 7\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.20, random_state=7)\npipeline = make_pipeline(StandardScaler(), SVC(random_state=0)) #first we scale, then run our classifier\npipeline.fit(x_train, y_train)\naccuracy = pipeline.score(x_test, y_test)\n\nprint('Accuracy for random_state=7 is',str(np.round(accuracy*100,2))+'%')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:36.45969Z","iopub.execute_input":"2021-08-09T18:20:36.460037Z","iopub.status.idle":"2021-08-09T18:20:36.475394Z","shell.execute_reply.started":"2021-08-09T18:20:36.460005Z","shell.execute_reply":"2021-08-09T18:20:36.474711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What should we do?\n\nWe should report the cross validation results. Furthermore, we should create as many folds as possible. For this, we are going to use `LeaveOneOut` cross validation. \n\nThat is, the test set will consist of only one row, and row will be used as a test set once and only once.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import LeaveOneOut\n\nloo = LeaveOneOut()\npipeline = make_pipeline(StandardScaler(), SVC(random_state=0))\n\ncv_result = cross_val_score(pipeline, x, y, cv=loo)\nprint('Leave-One-Out cross validation accuracy is', str(np.round(100*np.mean(cv_result),2))+'%')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:37.725098Z","iopub.execute_input":"2021-08-09T18:20:37.725456Z","iopub.status.idle":"2021-08-09T18:20:38.820271Z","shell.execute_reply.started":"2021-08-09T18:20:37.725427Z","shell.execute_reply":"2021-08-09T18:20:38.81935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Difficult points: (from the figure above)', difficult_points)\nprint('Misclassifications:',np.where(cv_result==0)[0].tolist())","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:20:41.987986Z","iopub.execute_input":"2021-08-09T18:20:41.98837Z","iopub.status.idle":"2021-08-09T18:20:41.994655Z","shell.execute_reply.started":"2021-08-09T18:20:41.988335Z","shell.execute_reply":"2021-08-09T18:20:41.993545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that our misclassification errors appeared where we have guessed!","metadata":{}}]}